[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
![Docker](https://img.shields.io/badge/Docker-28.3.0-blue?logo=docker)
![VSCode](https://img.shields.io/badge/VSCode-1.103.2-blue?logo=visual-studio-code)
![Python](https://img.shields.io/badge/Python-3.10.12-blue?logo=python)

# ğŸ›©ï¸ Agente RAG para Asistir al Piloto por Comandos de Voz - TFM

Este proyecto consiste en la creaciÃ³n de un asistente para el piloto, que permite la comunicaciÃ³n por voz y por escrito. El sistema abarca desde el reconocimiento de voz (Speech to Text) hasta el agente RAG, encargado de entender la consulta y generar la respuesta utilizando la base de datos proporcionada.

![Diagrama general](docs/diagramas/diagrama_v3_070925.png)
---

## â­ Consideraciones principales

- **ğŸ–¥ï¸ EjecuciÃ³n local:** Todo el sistema funciona completamente offline, sin conexiÃ³n a internet, garantizando la seguridad y la integraciÃ³n en sistemas aeronÃ¡uticos.
- **ğŸ§© Arquitectura de microservicios:** Cada microservicio realiza una acciÃ³n especÃ­fica.
- **ğŸ’» Desarrollo:** El proyecto se ha desarrollado en Visual Studio Code sobre Ubuntu WSL.

---

## âš™ï¸ InstalaciÃ³n inicial

Antes de comenzar, asegÃºrate de tener instalado lo siguiente:

1. **ğŸ“ Visual Studio Code**  
   DescÃ¡rgalo desde [Visual Studio Code](https://code.visualstudio.com/download)

2. **ğŸ§ Ubuntu WSL** (si trabajas desde Windows)  
   - [GuÃ­a de instalaciÃ³n de WSL](https://learn.microsoft.com/es-es/windows/wsl/install)

3. **ğŸ”— Git**
   - InstalaciÃ³n:
     ```bash
     sudo apt install git
     ```
   - Configura tu usuario de Git:
     ```bash
     git config --global user.name "Tu Nombre"
     git config --global user.email "tu@email.com"
     ```
   - (Recomendado) Configura autenticaciÃ³n SSH para mayor seguridad:
     ```bash
     ssh-keygen -t ed25519 -C "tu@email.com"
     ```
     Durante la generaciÃ³n de la clave, puedes establecer una **passphrase** (contraseÃ±a) para proteger tu clave privada SSH.  
     Si la estableces, cada vez que uses la clave se te pedirÃ¡ la contraseÃ±a.
     ```bash
     eval "$(ssh-agent -s)"
     ssh-add ~/.ssh/id_ed25519
     cat ~/.ssh/id_ed25519.pub
     ```
     Copia la clave pÃºblica y aÃ±Ã¡dela a tu cuenta de GitHub en [SSH keys](https://github.com/settings/keys).

4. **ğŸ Python 3.x y entorno virtual (obligatorio)**

   - **En Linux/WSL:**  
     Instala Python y pip:
     ```bash
     sudo apt update && sudo apt install python3 python3-pip python3.10-venv
     ```
     Crea y activa un entorno virtual:
     ```bash
     python3 -m venv venv
     source venv/bin/activate
     ```
     Para desactivar el entorno virtual:
     ```bash
     deactivate
     ```

   - **En Windows:**  
     Descarga e instala Python desde [python.org](https://www.python.org/downloads/).  
     Luego, en CMD:
     ```cmd
     python -m venv venv
     venv\Scripts\activate
     ```
     En PowerShell:
     ```powershell
     python -m venv venv
     .\venv\Scripts\Activate.ps1
     ```
     Para desactivar el entorno virtual (CMD o PowerShell):
     ```cmd
     deactivate
     ```

   > **â„¹ï¸ Nota:** Recuerda activar el entorno virtual cada vez que vayas a trabajar en el proyecto y desactivarlo con `deactivate` cuando hayas terminado.
5. **Carga automÃ¡tica de LLM por microservicio Ollama**

    El modelo LLM (por ejemplo, Mistral-7B-Instruct) se descarga automÃ¡ticamente por el microservicio de Ollama al arrancar, segÃºn la configuraciÃ³n indicada en el archivo `config.yaml` correspondiente. No es necesario descargar el modelo manualmente ni gestionar tokens de HuggingFace en la mayorÃ­a de los casos.

    > **Nota:** El microservicio Ollama gestiona la descarga y actualizaciÃ³n del modelo de forma automÃ¡tica. Solo asegÃºrate de que el archivo de configuraciÃ³n (`config.yaml`) especifica el modelo deseado y que el contenedor tiene acceso a internet la primera vez que se arranca para descargar el modelo (despuÃ©s se podrÃ¡ utilizar sin internet).

    > **Nota adicional:** Si el modelo LLM que deseas utilizar requiere permisos especiales de HuggingFace (por ejemplo, Mistral-7B-Instruct), es posible que debas:
    > - Crear una cuenta en HuggingFace en https://huggingface.co/join
    > - Solicitar acceso al modelo desde la pÃ¡gina correspondiente (ejemplo: Mistral-7B-Instruct-v0.3)
    > - Obtener tu token personal en https://huggingface.co/settings/tokens
    > - Autenticarte en tu terminal ejecutando:
    >   ```bash
    >   huggingface-cli login
    >   ```
    >   e introducir tu token cuando lo solicite.
    > Estos pasos solo son necesarios si Ollama o el microservicio lo solicita explÃ­citamente al descargar el modelo.
6. **Soporte GPU para Ollama y microservicios IA**
    Para usar Ollama (y otros microservicios de IA) con aceleraciÃ³n GPU, debes instalar el NVIDIA Container Toolkit en tu sistema host. Los pasos de instalaciÃ³n y configuraciÃ³n de Docker para GPU son los siguientes:

    #### Instalar NVIDIA Container Toolkit (Debian/Ubuntu)

      **Configura el repositorio:**
      ```bash
      curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      sudo apt-get update
      ```

      **Instala el toolkit:**
      ```bash
      sudo apt-get install -y nvidia-container-toolkit
      ```

      **Configura Docker para usar el driver Nvidia:**
      ```bash
      sudo nvidia-ctk runtime configure --runtime=docker
      sudo systemctl restart docker
      ```

      > **Nota:** El arranque de Ollama y el resto de microservicios se realiza mediante docker-compose, que ya incluye la configuraciÃ³n para usar GPU si estÃ¡ disponible. No es necesario arrancar manualmente los contenedores ni ejecutar modelos manualmente, salvo para pruebas avanzadas.
---

## ğŸš€ Uso del proyecto

1. Clona este repositorio (cambia *usuario* por tu usuario de GitHub):

   - Por HTTPS:
     ```bash
     git clone https://github.com/usuario/TFM-RAG-agent-for-pilot-by-voice-commands.git
     ```

   - Por SSH (requiere clave configurada):
     ```bash
     git clone git@github.com:usuario/TFM-RAG-agent-for-pilot-by-voice-commands.git
     ```

2. Activa el entorno virtual (ver instrucciones arriba).
3. Las dependencias de cada microservicio se instalan automÃ¡ticamente al construir los contenedores Docker. No es necesario instalar un requirements.txt global.
  > **â„¹ï¸ Nota:** Para consultar las Ãºltimas versiones de las librerÃ­as puedes visitar [PyPI](https://pypi.org/)
4. Antes de ejecutar los microservicios, puedes personalizar la configuraciÃ³n global del sistema editando el archivo `config.yaml` dentro de la carpeta `infrastructure`.

5. Ejecuta todos los microservicios mediante docker-compose:
  ```bash
  cd infrastructure
  docker compose up --build
  ```

  ### ğŸ³ Comandos bÃ¡sicos de Docker Compose

  Algunos comandos Ãºtiles para gestionar los microservicios:


  - `docker compose restart` â€” Reinicia todos los contenedores definidos en el archivo docker-compose.
  - `docker compose stop` â€” Detiene todos los contenedores sin eliminarlos.
  - `docker compose start` â€” Inicia los contenedores que han sido detenidos previamente.
  - `docker compose down` â€” Detiene y elimina todos los contenedores, redes y volÃºmenes definidos en el archivo docker-compose.
  - `docker compose up <servicio>` â€” Levanta Ãºnicamente el contenedor del servicio especificado (por ejemplo, `docker compose up rag`).
  - `docker ps -a` â€” Muestra todos los contenedores (activos e inactivos) en tu sistema Docker, no solo los de este proyecto.
  - `docker rm <id>` â€” Elimina un contenedor por su ID (puedes obtener el ID con `docker ps -a`).

  > Puedes consultar la lista de servicios disponibles en el archivo `docker-compose.yml` dentro de la carpeta `infrastructure`.

6. Si ejecutas el sistema por segunda vez y sabes que no hay nuevos archivos para analizar, puedes omitir la inicializaciÃ³n del microservicio de ingestion para ahorrar recursos. Basta con comentar o eliminar la referencia a ingestion en el archivo `docker-compose.yml` antes de lanzar los servicios.

---

## ğŸ—‚ï¸ Estructura del proyecto


A continuaciÃ³n, se muestra la estructura real y actual del proyecto:

```
ğŸ“ common/                   # ğŸ”— Utilidades y modelos compartidos
â”‚   â””â”€â”€vector_db/                # ğŸ—ƒï¸ Carpeta donde se guarda la base de datos vectorial local
ğŸ“ docs/                     # ğŸ“š DocumentaciÃ³n y ejemplos
â”‚   â”œâ”€â”€ audio_examples/          # ğŸµ Ejemplos de audio
â”‚   â”œâ”€â”€ dataset_procedures/      # ğŸ“„ Archivos que se incluirÃ¡n/analizarÃ¡n en la db
â”‚   â””â”€â”€ error_codes_api/         # ğŸ›‘ CÃ³digos de error de la API

ğŸ“ infrastructure/           # ğŸ—ï¸ OrquestaciÃ³n y configuraciÃ³n global
â”‚   â”œâ”€â”€ docker-compose.yml      # ğŸ³ OrquestaciÃ³n principal de todos los microservicios
â”‚   â””â”€â”€ config.yaml             # âš™ï¸ ConfiguraciÃ³n global editable de los MS

ğŸ“ notebooks/                # ğŸ““ Jupyter Notebooks para experimentaciÃ³n
â”‚   â””â”€â”€ prueba_general.ipynb     # ğŸ“Š Notebook de pruebas generales

ğŸ“ scripts/                  # ğŸ§ª Scripts de utilidad y pruebas
â”‚   â”œâ”€â”€ prueba_db.py             # ğŸ” Muestra y explora los documentos almacenados en la vector DB
â”‚   â”œâ”€â”€ search_db.py             # ğŸ” Realiza bÃºsquedas semÃ¡nticas en la vector DB usando ChromaDB
â”‚   â”œâ”€â”€ forzar_eliminar_path.py  # ğŸ—‘ï¸ Elimina carpetas y __pycache__ de forma forzada
â”‚   â”œâ”€â”€ rag_basic_and db.py      # ğŸ§© Prueba chunking y carga de documentos con Docling y LangChain
â”‚   â””â”€â”€ descarga_llm_mistal.py   # â¬‡ï¸ Descarga el modelo Mistral-7B-Instruct desde HuggingFace

ğŸ“ services/                # ğŸ§© Microservicios principales
â”‚   â”œâ”€â”€ asr/                 # ğŸ—£ï¸ Microservicio de reconocimiento de voz (Whisper)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py                  # ğŸš€ Punto de entrada del microservicio ASR
â”‚   â”‚   â”‚   â”œâ”€â”€ transcribers/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ whisper.py          # ğŸ—£ï¸ LÃ³gica de transcripciÃ³n con Whisper
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â”‚       â””â”€â”€ logger.py           # ğŸ“ ConfiguraciÃ³n y utilidades de logging
â”‚   â”‚   â”œâ”€â”€ Dockerfile           # ğŸ³ Imagen Docker de ASR
â”‚   â”‚   â””â”€â”€ requirements.txt     # ğŸ“¦ Dependencias de ASR
â”‚   â”œâ”€â”€ ingestion/           # ğŸ“¥ Microservicio de ingesta y procesamiento de documentos
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ watcher.py              # ğŸ‘€ Observador de cambios en carpetas de documentos
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentHandler/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ pdf_handler.py      # ğŸ“„ Procesamiento especÃ­fico de PDFs
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chunker.py          # âœ‚ï¸ LÃ³gica de segmentaciÃ³n (chunking) de texto
â”‚   â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chromadb_repository.py # ğŸ—ƒï¸ Acceso y gestiÃ³n de la base de datos vectorial
â”‚   â”‚   â”‚   â”œâ”€â”€ embedders/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ sentence_transformers_embedders.py # ğŸ”— GeneraciÃ³n de embeddings con Sentence Transformers
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ingest_runner.py    # ğŸƒ OrquestaciÃ³n del proceso de ingesta
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ocr_service.py      # ğŸ” Servicio de reconocimiento Ã³ptico de caracteres
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â”‚       â””â”€â”€ logger.py           # ğŸ“ ConfiguraciÃ³n y utilidades de logging
â”‚   â”‚   â”œâ”€â”€ Dockerfile           # ğŸ³ Imagen Docker de Ingesta
â”‚   â”‚   â””â”€â”€ requirements.txt     # ğŸ“¦ Dependencias de Ingesta
â”‚   â”œâ”€â”€ ollama/              # ğŸ¤– Microservicio LLM (Ollama)
â”‚   â”‚   â”œâ”€â”€ Dockerfile           # ğŸ³ Imagen Docker de Ollama
â”‚   â”‚   â””â”€â”€ entrypoint.sh        # âš¡ Script de arranque del contenedor Ollama
â”‚   â”œâ”€â”€ rag/                 # ğŸ“š Microservicio RAG (Retrieval-Augmented Generation)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py                   # ğŸš€ Punto de entrada del microservicio RAG
â”‚   â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RAG.py               # ğŸ§  LÃ³gica principal del agente RAG
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ReActAgent.py        # ğŸ¤– ImplementaciÃ³n del agente ReAct
â”‚   â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chromadb_repository.py # ğŸ—ƒï¸ Acceso y gestiÃ³n de la base de datos vectorial
â”‚   â”‚   â”‚   â”œâ”€â”€ embedders/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ sentence_transformers_embedders.py # ğŸ”— GeneraciÃ³n de embeddings con Sentence Transformers
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ollama_service.py     # ğŸ¤– Servicio de conexiÃ³n con Ollama
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ *.txt                 # ğŸ’¬ Prompts en texto plano
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.py                  # ğŸ’¬ LÃ³gica para abrir los prompts
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â”‚       â””â”€â”€ logger.py             # ğŸ“ ConfiguraciÃ³n y utilidades de logging
â”‚   â”‚   â”œâ”€â”€ Dockerfile           # ğŸ³ Imagen Docker del RAG
â”‚   â”‚   â””â”€â”€ requirements.txt     # ğŸ“¦ Dependencias del RAG
â”‚   â””â”€â”€ streamlit/           # ğŸ–¥ï¸ Frontend visual (Streamlit)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ app.py                   # ğŸš€ Punto de entrada del frontend
â”‚       â”‚   â””â”€â”€ utils/
â”‚       â”‚       â””â”€â”€ logger.py            # ğŸ“ ConfiguraciÃ³n y utilidades de logging
â”‚       â”œâ”€â”€ Dockerfile           # ğŸ³ Imagen Docker del frontend
â”‚       â””â”€â”€ requirements.txt     # ğŸ“¦ Dependencias del frontend

ğŸ“ .gitignore                # ğŸš« Ignorar archivos/carpetas
ğŸ“ README.md                 # ğŸ“˜ README principal
```

---

## ğŸ§© DescripciÃ³n detallada de los microservicios y componentes


### 1. Microservicio ASR (Automatic Speech Recognition)
Encargado de convertir audio en texto utilizando el modelo Whisper. Recibe archivos de audio, los procesa y devuelve la transcripciÃ³n. Se comunica principalmente con el frontend (Streamlit) y puede enviar resultados al microservicio RAG para su procesamiento.

### 2. Microservicio Ingestion
Responsable de la ingesta y procesamiento de documentos. Observa cambios en la carpeta `docs/dataset_procedures/`, realiza OCR en PDFs, segmenta el texto (chunking) y genera embeddings con Sentence Transformers. Los documentos procesados se almacenan en la base de datos vectorial (ChromaDB).

### 3. Microservicio RAG (Retrieval-Augmented Generation)
Implementa dos tipos de agentes para la recuperaciÃ³n y generaciÃ³n de respuestas:
  - **RAG bÃ¡sico:** Recupera informaciÃ³n relevante de la base de datos vectorial y genera una respuesta basada en los documentos encontrados.
  - **Agente ReAct:** Utiliza un enfoque de razonamiento y acciÃ³n, combinando recuperaciÃ³n de informaciÃ³n y generaciÃ³n explicativa.
Ambos agentes se conectan con ChromaDB para obtener los chunks relevantes y con Ollama para la generaciÃ³n de texto.

### 4. Microservicio Ollama (LLM)
Encargado de descargar y servir el modelo LLM (por ejemplo, Mistral-7B-Instruct). El modelo se descarga automÃ¡ticamente al arrancar el contenedor segÃºn la configuraciÃ³n en `config.yaml`. Ollama se comunica con el microservicio RAG para la generaciÃ³n de respuestas.

### 5. Microservicio Streamlit (Frontend)
Proporciona la interfaz visual para interactuar con el sistema. Permite enviar consultas por voz o texto, visualizar respuestas y estadÃ­sticas. Se conecta con los microservicios ASR y RAG.

### 6. ChromaDB (Base de datos vectorial)
No aparece como carpeta propia en la estructura porque se monta automÃ¡ticamente como servicio en Docker Compose. ChromaDB almacena los embeddings generados por el microservicio Ingestion y es consultada por los agentes RAG/ReAct para recuperar informaciÃ³n relevante. Los datos se guardan en la carpeta local `common/vector_db/` y en el propio Docker.

---


## â— Puntos de mejora y tareas pendientes

- Revisar que el microservicio de Ingestion ignore correctamente los documentos ya presentes en la base de datos.
- Probar la ingesta y recuperaciÃ³n con documentos mÃ¡s sencillos (ejemplo: PDF bÃ¡sico sin tablas).
- Incorporar otros tipos de archivos, no solo PDFs.
- El RAG bÃ¡sico no devuelve informaciÃ³n de la base de datos correctamente; revisar la lÃ³gica de recuperaciÃ³n y respuesta o mÃ¡s bien como se guarda en la db.
- El agente ReAct puede generar explicaciones que no provienen de la base de datos; mejorar la integraciÃ³n y control de fuentes.
- AÃ±adir en las estadÃ­sticas de Streamlit un parÃ¡metro que muestre el tiempo de respuesta medio segÃºn el tipo de agente (RAG/ReAct) y el total.
- "Profesionalizar" el microservicio de StreamLit: asegurar los tipos de I/O de los mÃ©todos, crear clases/mÃ©todos y que no sea tan largo el scripts. Mejorar la parte visual.
- Incluir y documentar la estructura y organizaciÃ³n basada en Domain Driven Design (DDD), mostrando cÃ³mo se distribuyen los dominios, entidades, servicios y mÃ³dulos principales del sistema.

---

## ğŸ¤ ContribuciÃ³n
Dicho proyecto se ha inicializado gracias al proyecto de la asignatura de AviÃ³nica Avanzada del MÃ¡ster Universitario en IngenierÃ­a AeronÃ¡utica de la Universidad de Sevilla y de los proyectos internos del grupo empresarial OesÃ­a, en concreto al Centro de Competencias de Inteligencia Artificial & Data.

Si deseas contribuir, por favor abre un issue o un pull request.

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la licencia [Apache License 2.0](./LICENSE).
